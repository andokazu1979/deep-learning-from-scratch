{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4.1 乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n",
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "print(price)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4.2 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# layer\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# forward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.1 ReLU レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "x & (x>0) \\\\\n",
    "0 & (x\\leqq0) \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\left\\{\n",
    "\\begin{array}{ll}\n",
    "1 & (x>0) \\\\\n",
    "0 & (x\\leqq0) \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5.2 Sigmoid レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=\\frac{1}{1+exp(-x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=y^{2}exp(-x)=y(1-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine / Softmax レイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.1 Affine レイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bf Y} =\n",
    "( \\frac{\\partial L}{\\partial y_{1}},\\frac{\\partial L}{\\partial y_{2}},\\frac{\\partial L}{\\partial y_{3}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\bf W} =\n",
    "\\begin{pmatrix}\n",
    "w_{11} &w_{21} &w_{31}\\\\\n",
    "w_{12} &w_{22} &w_{32}\n",
    "\\end{pmatrix}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\bf X} = ( x_{0},x_{1} )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bf X} =\n",
    "    ( \\frac{\\partial L}{\\partial x_{1}},\\frac{\\partial L}{\\partial x_{2}})\\\\=( \\frac{\\partial L}{\\partial y_{1}}w_{11}+\\frac{\\partial L}{\\partial y_{2}}w_{21}+\\frac{\\partial L}{\\partial y_{3}}w_{31}, \\frac{\\partial L}{\\partial y_{1}}w_{12}+\\frac{\\partial L}{\\partial y_{2}}w_{22}+\\frac{\\partial L}{\\partial y_{3}}w_{32} )\\\\=\\frac{\\partial L}{\\partial \\bf Y} \\cdot {\\bf W^{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\bf X}=\\frac{\\partial L}{\\partial \\bf Y}\\cdot {\\bf W}^{T} \\\\\n",
    "\\frac{\\partial L}{\\partial \\bf W}={\\bf X}^{T}\\cdot \\frac{\\partial L}{\\partial \\bf Y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.2 バッチ版 Affine レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W ,b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6.3 Softmax-with-Loss レイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 誤差逆伝播法の実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.2 誤差逆伝播法に対応したニューラルネットワークの 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = \\\n",
    "            Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = \\\n",
    "            Affine(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "            \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.3 誤差逆伝播法の勾配確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.01085902782e-13\n",
      "b1:9.17005403361e-13\n",
      "W2:7.9879719633e-13\n",
      "b2:1.2012613404e-10\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "    \n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \":\" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7.4 誤差逆伝播法を使った学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0\n",
      "0.147083333333 0.1486\n",
      "Iteration : 600\n",
      "0.903333333333 0.9084\n",
      "Iteration : 1200\n",
      "0.920816666667 0.9245\n",
      "Iteration : 1800\n",
      "0.929683333333 0.9297\n",
      "Iteration : 2400\n",
      "0.94365 0.9412\n",
      "Iteration : 3000\n",
      "0.95055 0.9469\n",
      "Iteration : 3600\n",
      "0.957383333333 0.9534\n",
      "Iteration : 4200\n",
      "0.95935 0.9556\n",
      "Iteration : 4800\n",
      "0.9643 0.9589\n",
      "Iteration : 5400\n",
      "0.968116666667 0.9622\n",
      "Iteration : 6000\n",
      "0.970333333333 0.964\n",
      "Iteration : 6600\n",
      "0.9705 0.9635\n",
      "Iteration : 7200\n",
      "0.9736 0.9658\n",
      "Iteration : 7800\n",
      "0.974216666667 0.9654\n",
      "Iteration : 8400\n",
      "0.97555 0.9675\n",
      "Iteration : 9000\n",
      "0.975883333333 0.9673\n",
      "Iteration : 9600\n",
      "0.979216666667 0.9701\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(normalize=True, one_hot_label=True)\n",
    "    \n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        print(\"Iteration : {0}\".format(i))\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x119786cc0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1197dc400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_acc_list, lapack_litebel=\"train\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
